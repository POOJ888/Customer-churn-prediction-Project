{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3c18654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data DATA] [--target TARGET]\n",
      "                             [--model MODEL] [--test_size TEST_SIZE]\n",
      "                             [--save_dir SAVE_DIR] [--predict PREDICT]\n",
      "                             [--model_path MODEL_PATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=c:\\Users\\Hp\\AppData\\Roaming\\jupyter\\runtime\\kernel-v385f5e451f6670c4a935d292b217a2dba180d9465.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hp\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "customer_churn.py\n",
    "=================\n",
    "A single‑file, end‑to‑end machine‑learning workflow for Customer Churn Prediction.\n",
    "\n",
    "Usage examples\n",
    "--------------\n",
    "# Train on a CSV and evaluate on a held‑out test split\n",
    "python customer_churn.py --data data/telco.csv --target Churn --test_size 0.2 --model xgboost\n",
    "\n",
    "# Predict churn probability for a JSON record (after training)\n",
    "python customer_churn.py --predict sample_customer.json --model_path models/best_model.pkl --target Churn\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "- pandas\n",
    "- numpy\n",
    "- scikit‑learn\n",
    "- imbalanced‑learn\n",
    "- xgboost (optional but recommended)\n",
    "- joblib\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier  # type: ignore\n",
    "    _HAS_XGB = True\n",
    "except ImportError:  # pragma: no cover\n",
    "    _HAS_XGB = False\n",
    "    XGBClassifier = None  # type: ignore\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "\n",
    "def load_data(path: str | Path) -> pd.DataFrame:\n",
    "    \"\"\"Load a CSV file into a DataFrame.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_X_y(df: pd.DataFrame, target: str) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Separate features ``X`` and label ``y``.\"\"\"\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target}' not found in data.\")\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target].copy()\n",
    "    if y.dtype == \"object\":\n",
    "        y = y.map({\"Yes\": 1, \"No\": 0}).fillna(y)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def build_preprocessor(X: pd.DataFrame) -> ColumnTransformer:\n",
    "    \"\"\"Return a ColumnTransformer that preprocesses numeric & categorical columns.\"\"\"\n",
    "    numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "    categorical_cols = X.select_dtypes(exclude=[\"int64\", \"float64\"]).columns.tolist()\n",
    "\n",
    "    numeric_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    categorical_pipeline = Pipeline([\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"num\", numeric_pipeline, numeric_cols),\n",
    "        (\"cat\", categorical_pipeline, categorical_cols),\n",
    "    ])\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def get_model(name: str) -> Tuple[str, object]:\n",
    "    \"\"\"Return (name, estimator) pair for the requested model.\"\"\"\n",
    "    name = name.lower()\n",
    "    if name == \"logreg\":\n",
    "        return \"logreg\", LogisticRegression(max_iter=1000, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "    if name == \"rf\":\n",
    "        return \"rf\", RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE)\n",
    "    if name == \"gb\":\n",
    "        return \"gb\", GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
    "    if name == \"xgboost\":\n",
    "        if not _HAS_XGB:\n",
    "            raise ImportError(\"xgboost is not installed. pip install xgboost\")\n",
    "        return \"xgboost\", XGBClassifier(\n",
    "            n_estimators=500,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=RANDOM_STATE,\n",
    "            objective=\"binary:logistic\",\n",
    "        )\n",
    "    raise ValueError(f\"Unknown model '{name}'. Choose from logreg, rf, gb, xgboost.\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test: pd.DataFrame, y_test: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"Return common classification metrics as a dict.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
    "    }\n",
    "    if y_prob is not None:\n",
    "        metrics[\"roc_auc\"] = roc_auc_score(y_test, y_prob)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    metrics[\"tn\"], metrics[\"fp\"], metrics[\"fn\"], metrics[\"tp\"] = cm.ravel()\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def pretty_print_metrics(metrics: Dict[str, float]):\n",
    "    header = (\n",
    "        f\"Accuracy:  {metrics['accuracy']:.3f}\\n\"\n",
    "        f\"Precision: {metrics['precision']:.3f}\\n\"\n",
    "        f\"Recall:    {metrics['recall']:.3f}\\n\"\n",
    "        f\"F1‑score:  {metrics['f1']:.3f}\"\n",
    "    )\n",
    "    if \"roc_auc\" in metrics:\n",
    "        header += f\"\\nROC‑AUC:  {metrics['roc_auc']:.3f}\"\n",
    "    print(header)\n",
    "    print(\"Confusion matrix [TN FP; FN TP] ⇒\", metrics[\"tn\"], metrics[\"fp\"], metrics[\"fn\"], metrics[\"tp\"])\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "    df: pd.DataFrame,\n",
    "    target: str,\n",
    "    model_name: str,\n",
    "    test_size: float = 0.2,\n",
    ") -> Tuple[Pipeline, Dict[str, float]]:\n",
    "    X, y = split_X_y(df, target)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    preprocessor = build_preprocessor(X_train)\n",
    "    model_label, clf = get_model(model_name)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        (\"pre\", preprocessor),\n",
    "        (model_label, clf),\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(pipe, X_test, y_test)\n",
    "    return pipe, metrics\n",
    "\n",
    "\n",
    "def predict_single(model_path: str | Path, json_path: str | Path):\n",
    "    model = joblib.load(model_path)\n",
    "    with open(json_path) as f:\n",
    "        record = json.load(f)\n",
    "    df = pd.DataFrame([record])\n",
    "    prob = model.predict_proba(df)[0][1]\n",
    "    label = int(prob >= 0.5)\n",
    "    print(f\"Churn probability: {prob:.3f} → Label: {label}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"Customer Churn Prediction\")\n",
    "    parser.add_argument(\"--data\", type=str, help=\"Path to CSV data file\")\n",
    "    parser.add_argument(\"--target\", type=str, default=\"Churn\", help=\"Target column name\")\n",
    "    parser.add_argument(\"--model\", type=str, default=\"xgboost\", help=\"Model: logreg, rf, gb, xgboost\")\n",
    "    parser.add_argument(\"--test_size\", type=float, default=0.2, help=\"Test size fraction\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"models\", help=\"Directory to save trained model\")\n",
    "    parser.add_argument(\"--predict\", type=str, help=\"Path to JSON file for single prediction\")\n",
    "    parser.add_argument(\"--model_path\", type=str, help=\"Path to saved model for prediction\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.predict and args.model_path:\n",
    "        predict_single(args.model_path, args.predict)\n",
    "        sys.exit()\n",
    "\n",
    "    if not args.data:\n",
    "        parser.error(\"--data is required unless using --predict with --model_path\")\n",
    "\n",
    "    df = load_data(args.data)\n",
    "    print(f\"Loaded data shape: {df.shape}\")\n",
    "\n",
    "    pipe, metrics = train_and_evaluate(df, args.target, args.model, args.test_size)\n",
    "    pretty_print_metrics(metrics)\n",
    "\n",
    "    Path(args.save_dir).mkdir(exist_ok=True)\n",
    "    model_fp = Path(args.save_dir) / \"best_model.pkl\"\n",
    "    joblib.dump(pipe, model_fp)\n",
    "    print(f\"Model saved to {model_fp.resolve()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
